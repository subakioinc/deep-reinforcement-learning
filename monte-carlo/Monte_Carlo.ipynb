{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo 방법\n",
    "\n",
    "이 notebook에서는 다양한 Monte Carlo(MC) 알고리즘을 직접 구현해본다.\n",
    "\n",
    "시작에 필요한 코드를 제공하지만 힌트들 지우고 직접 처음부터 작성해보는 것도 추천한다.\n",
    "\n",
    "### Part 0: BlackjackEnv 경험해보기\n",
    "\n",
    "시작은 필요한 package를 import 시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "from plot_utils import plot_blackjack_values, plot_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드를 사용하여 [Blackjack](https://github.com/openai/gym/blob/master/gym/envs/toy_text/blackjack.py) environment의 인스턴스를 생성."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Blackjack-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 state는 다음과 같은 3-tuple 로 구성:\n",
    "- the player's current sum $\\in \\{0, 1, \\ldots, 31\\}$,\n",
    "- the dealer's face up card $\\in \\{1, \\ldots, 10\\}$, and\n",
    "- whether or not the player has a usable ace (`no` $=0$, `yes` $=1$).\n",
    "\n",
    "agent는 2가지 가능한 actions을 가진다:\n",
    "\n",
    "```\n",
    "    STICK = 0\n",
    "    HIT = 1\n",
    "```\n",
    "아래 코드를 실행하여 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 cell 코드를 실행하면 random polity로 Blackjack을 play한다.\n",
    "\n",
    "(_아래 코드는 Blackjack을 3번 play한다. - 이 숫자를 바꾸거나 아래 cell을 여러번 실행할 수 있다.  해당  cell은 agent가 environment와 상호작용하여 반환되는 결과를 경험할 수 있도록 구현하였다._)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_episode in range(3):\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        print(state)\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print('End game! Reward: ', reward)\n",
    "            print('You won :)\\n') if reward > 0 else print('You lost :(\\n')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: MC Prediction\n",
    "\n",
    "이 섹션에서 MC prediction(action-value function을 estimation하기) 구현부를 직접 작성해보자.\n",
    "\n",
    "player는 카드의 합이 18을 넘으면 _거의_ 항상 stick되는 policy를 보게된다. 특히 sum이 18을 넘으면 `STICK` action을 80% 확률로 선택한다. sum이 18 혹은 아래면 80% 확률로 `HIT` action을 선택한다. `generate_episode_from_limit_stochastic` 함수는 이 policy를 사용하여 episode를 샘플한다.\n",
    "\n",
    "이 함수는 **input**으로 :\n",
    "- `bj_env`: OpenAI Gym의 Blackjack environment의 인스턴스이다.\n",
    "\n",
    "**output**으로 반환 :\n",
    "- `episode`: This is a list of (state, action, reward) tuples (of tuples) and corresponds to $(S_0, A_0, R_1, \\ldots, S_{T-1}, A_{T-1}, R_{T})$, where $T$ is the final time step.  In particular, `episode[i]` returns $(S_i, A_i, R_{i+1})$, and `episode[i][0]`, `episode[i][1]`, and `episode[i][2]` return $S_i$, $A_i$, and $R_{i+1}$, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode_from_limit_stochastic(bj_env):\n",
    "    episode = []\n",
    "    state = bj_env.reset()\n",
    "    while True:\n",
    "        probs = [0.8, 0.2] if state[0] > 18 else [0.2, 0.8]\n",
    "        action = np.random.choice(np.arange(2), p=probs)\n",
    "        next_state, reward, done, info = bj_env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "해당 policy로 Blackjack을 play하기 위해서 아래 코드 cell을 실행.\n",
    "\n",
    "(*이 코드는 Blackjack을 3번 play한다. - 이 숫자를 바꾸거나 cell을 여러번 실행한다.  `generate_episode_from_limit_stochastic` 함수의 출력에 대해서 친숙해지라고 이 cell을 제공한다.*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print(generate_episode_from_limit_stochastic(env))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 MC prediciton의 구현부를 직접 작성할 준비가 되었다. first-visit이나 every-visit MC prediction 중에 하나를 자유롭게 구현해보자. Blackjack environment의 경우도 동일한 기술은 동일하다.\n",
    "\n",
    "알고리즘은 다음의 3개 인자를 갖는다:\n",
    "- `env`: OpenAI Gym environment의 instance.\n",
    "- `num_episodes`: episode의 개수로 agent-environment 상호작용을 통해서 생성.\n",
    "- `generate_episode`: 상호작용 episode의 반환하는 함수\n",
    "- `gamma`: discount rate. 0과 1사이의 값을 갖는다. (default value: `1`).\n",
    "\n",
    "알고리즘이 출력으로 반환 :\n",
    "- `Q`: dictionary(1차원 배열)로 `Q[s][a]`는 state `s`와 action `a`에 대한 estimated action value이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_prediction_q(env, num_episodes, generate_episode, gamma=1.0):\n",
    "    # initialize empty dictionaries of arrays\n",
    "    returns_sum = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    N = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 1000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        ## TODO: complete the function\n",
    "        \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 cell을 이용해서 $Q$를 estimate하는 action-value function을 얻는다. state-value 함수를 그래프가 나온다.\n",
    "\n",
    "구현의 정확도를 검증하기 위해서 아래 그래프와 **Monte_Carlo_Solution.ipynb** 의 그래프를 비교해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the action-value function\n",
    "Q = mc_prediction_q(env, 500000, generate_episode_from_limit_stochastic)\n",
    "\n",
    "# obtain the corresponding state-value function\n",
    "V_to_plot = dict((k,(k[0]>18)*(np.dot([0.8, 0.2],v)) + (k[0]<=18)*(np.dot([0.2, 0.8],v))) \\\n",
    "         for k, v in Q.items())\n",
    "\n",
    "# plot the state-value function\n",
    "plot_blackjack_values(V_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: MC Control\n",
    "\n",
    "이 섹션에서 constant-$\\alpha$ MC control 를 직접 구현해보자.\n",
    "\n",
    "알고리즘의 4개 인자:\n",
    "- `env`: This is an instance of an OpenAI Gym environment.\n",
    "- `num_episodes`: This is the number of episodes that are generated through agent-environment interaction.\n",
    "- `alpha`: step을 업데이트하기 위한 step-size parameter\n",
    "- `gamma`: discount rate. 0과 1사이 값으로 (default value: `1`).\n",
    "\n",
    "알고리즘이 출력으로 반환 :\n",
    "- `Q`: This is a dictionary (of one-dimensional arrays) where `Q[s][a]` is the estimated action value corresponding to state `s` and action `a`.\n",
    "- `policy`: dictionary로 `policy[s]`는 state `s`를 observing한 후에 agent가 선택하는 action을 반환한다.\n",
    "\n",
    "(_원하면 추가 함수를 정의하여 작성 가능_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control(env, num_episodes, alpha, gamma=1.0):\n",
    "    nA = env.action_space.n\n",
    "    # initialize empty dictionary of arrays\n",
    "    Q = defaultdict(lambda: np.zeros(nA))\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 1000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        ## TODO: complete the function\n",
    "        \n",
    "    return policy, Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 cell을 사용하여 estimated optimal policy와 action-value function를 얻을 수 있다.  `num_episodes` 와 `alpha` parameters에는 직접 값을 넣어야만 하는 것을 명심하자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the estimated optimal policy and action-value function\n",
    "policy, Q = mc_control(env, ?, ?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로 corresponding state-value function 그래프를 그린다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# obtain the corresponding state-value function\n",
    "V = dict((k,np.max(v)) for k, v in Q.items())\n",
    "\n",
    "# plot the state-value function\n",
    "plot_blackjack_values(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로 최적으로 estimation된 policy를 시각화한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the policy\n",
    "plot_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**true** optimal policy $\\pi_*$는  [textbook](http://go.udacity.com/rl-textbook)의 Figure 5.2에서 볼 수 있다. 최종 optimal policy와 비교해보면 결과가 만족스러운지 확인하자. 알고리즘 성능에 만족하지 않는다면 decay rate of $\\epsilon$, $\\alpha$ 값을 변경하거나 더 많은 episode들에 대해서 알고리즘을 실행하여 더 낮은 결과를 얻을 수 있다.\n",
    "\n",
    "![True Optimal Policy](images/optimal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
